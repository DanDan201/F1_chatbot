# Ollama server (default local)
OLLAMA_HOST=http://localhost:11434

# Model names you have pulled with `ollama pull <model>`
LLM_MODEL=qwen3:4b
EMBED_MODEL=nomic-embed-text

# Retrieval parameters
CHUNK_SIZE=800
CHUNK_OVERLAP=100
TOP_K=10

# Where Chroma stores the vector DB
CHROMA_DIR=./chroma_db

# Bind address for Flask
HOST=0.0.0.0
PORT=5001

N_CONTEXT = 5
SECRET_KEY = Duyanh21